{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import sys\n",
    "import os\n",
    "import mmh3\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as sFuncs\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA5 Question 1 - Moving Averages\n",
    "\n",
    "Calculation of moving stock price averages are part of many a trading strategies ([reference](https://www.investopedia.com/articles/active-trading/052014/how-use-moving-average-buy-stocks.asp)). We will be using the two moving averages strategy, with the shorter-term MA being 10-day and the longer average being 40-day. When the shorter-term MA crosses above the longer-term MA, it's a buy signal, as it indicates that the trend is shifting up. This is known as a \"golden cross.\"\n",
    "\n",
    "Meanwhile, when the shorter-term MA crosses below the longer-term MA, it's a sell signal, as it indicates that the trend is shifting down. This is known as a \"dead/death cross.\"\n",
    "\n",
    "To simulate a data stream, you are given a python program `stream-feeder.py` which reads in `dj30.csv` file and pipes it, line by line. `dj30.csv` contains a 25-year history of the Dow Jones Industrial Average prices. We will only be concerned with the Close price. The command `stream-feeder.py | nc -lk 9999` can be run on the master machine of your spark cluster to feed the Close data into pyspark.\n",
    "\n",
    "1. Set up the stream to feed data into a pyspark DStream. Write and submit a summary of the steps you took (in English) and enclose the (cleaned up after editing) output of `history > /tmp/my_session.txt`. This history should include what you typed into the shell outside of the pyspark session. \\[2 pts\\]\n",
    "2. Use DStream windowing to separately accumulate the sum and count of prices, thus creating moving average DStreams. Write and submit the (cleaned up after editing) transcript of your session along with your code. \\[4 pts\\]\n",
    "3. \\[Optional, 4 bonus points\\]. Compare the two moving averages to indicate buy and sell signals. Your output should be of the form `[( <date> buy), ( <date> sell), etc]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "Note that this below cell needs to be run once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to unpack the dataset into the current directory\n",
    "# NOTE that this cell needs to run once\n",
    "# %%bash\n",
    "# cd /home/saberbf/BigData/PA5\n",
    "# sudo apt-get install python3-pip\n",
    "# pip3 install pandas\n",
    "# pip3 install feedparser\n",
    "# gsutil cp gs://datathinks-home/stream-feeder.py .\n",
    "# gsutil cp gs://datathinks-home/dj30.csv .\n",
    "# gsutil cp gs://datathinks-home/headline-extractor.py .\n",
    "# gsutil cp gs://datathinks-home/feed-parser.py .\n",
    "# gsutil cp gs://datathinks-home/2020-headlines.csv ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark Streaming Context\n",
    "\n",
    "##### Notes:\n",
    "- In case you are using a single-node with k many threads cluster, it is essential to use setMaster('local[k]') or less than k. Otherwise, SparkContext put the sc.master on 'yarn'. It doesn't consider threads as workers and looks for individual workers to do the job. The result would be you will not see a collect() to converge.\n",
    "- master is a Spark, Mesos or YARN cluster URL, or a special “local[*]” string to run in local mode. In practice, when running on a cluster, you will not want to hardcode master in the program, but rather launch the application with spark-submit and receive it there. However, for local testing and unit tests, you can pass “local[*]” to run Spark Streaming in-process (detects the number of cores in the local system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc._conf.getAll()\n",
    "sc.stop()\n",
    "# Create a local StreamingContext with 4 working threads\n",
    "conf = SparkConf().setMaster('local[*]')\n",
    "sc = SparkContext(conf=conf, appName='NetworkWordCount')\n",
    "\n",
    "# test the SparkContext to see if it works\n",
    "# rdd = sc.parallelize([('a',7),('a',2),('b',2)])\n",
    "# rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local StreamingContext with batch interval of 2.5 seconds which should \n",
    "# hold to 10 sample per RDD\n",
    "ssc = StreamingContext(sc, 0.25)\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \")).map(lambda word:(word, 1))\n",
    "# reduce last 30 seconds of data, every 10 seconds\n",
    "priceSum = words.reduceByWindow(\n",
    "    lambda x, y: ((float(x[0])+float(y[0]))/(int(x[1])+int(y[1])), (int(x[1])+int(y[1]))),\n",
    "    None,#lambda x, y: (float(x[0])-float(y[0]), int(x[1])-int(y[1])),\n",
    "    windowDuration=2.5,\n",
    "    slideDuration=0.25)\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "priceSum.pprint()\n",
    "# priceSum.saveAsTextFiles(\"hdfs:///results/MA10\")\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()#(stopSparkContext=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local StreamingContext with batch interval of 10 seconds which should hold to 40 sample per RDD\n",
    "ssc = StreamingContext(sc, 0.25)\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \")).map(lambda word:(word, 1))\n",
    "# reduce last 30 seconds of data, every 10 seconds\n",
    "priceSum = words.reduceByWindow(\n",
    "    lambda x, y: ((float(x[0])+float(y[0]))/(int(x[1])+int(y[1])), (int(x[1])+int(y[1]))),\n",
    "    None,#lambda x, y: (float(x[0])-float(y[0]), int(x[1])-int(y[1])),\n",
    "    windowDuration=4 * 2.5,\n",
    "    slideDuration=0.25)\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "priceSum.pprint()\n",
    "# priceSum.saveAsTextFiles(\"hdfs:///results/MA40\")\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()#(stopSparkContext=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra resources\n",
    "This part is to load the dataset and try to get it into another RDD. It shouldn't be included in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the data into a DataFrame\n",
    "spark = SparkSession.builder.master('local[4]').getOrCreate()\n",
    "dj30DF = spark.read.load('gs://datathinks-home/dj30.csv', \n",
    "                     format='csv', inferSchema=True, header=True, delimiter=',')\n",
    "dj30DF = dj30DF.select(['Close', \n",
    "           'Date', \n",
    "           'Long Date', \n",
    "           'Total Stks above 30MA', \n",
    "           'Total Stks below 30MA', \n",
    "           'Total Stks equal 30MA'])\n",
    "\n",
    "# dj30DF.withColumn('movingAverage', sum(dj30DF[Close])).over(Window.rowsBetween(-10,0)).show(10)\n",
    "dj30DF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA5 Question 2 - Notable News\n",
    "\n",
    "Most news outlets distribute their news through rss feeds for use by news reader programs. We are writing a news reader that reads the news headlines and only reports those headlines that contain an unfamiliar word. (It's not going to be all that useful but hey...)\n",
    "\n",
    "The file `2020-headlines.csv` contains headlines from 2020 for mining for familiar words and `headline-extractor.py` for extracting words from such headlines. The program is only half-written. Add to it as follows:\n",
    "\n",
    "1. Create a Bloom Filter string whose size is approximately 8 times the number of understood words and write the buffer into a text file `bloom.txt` in your shell.\n",
    "2. The file `bloom.txt` will be used in pyspark. You may want to store it in hdfs so it is accessible from pyspark.\n",
    "To simulate a data stream, you are given a python program `news-feeder.py` which reads rss feeds from several news outlets. It is rate controlled, feeding us 4 titles per second. The commands `news-feeder.py | nc -lk 9999` or `./news-feeder.py  | tee /dev/stderr | nc -lk 9999`can be run on the master machine of your spark cluster to feed the titles data into pyspark.\n",
    "\n",
    "1. Set up the stream to feed data into a pyspark DStream. Write and submit a summary of the steps you took (in English) and enclose the (cleaned up after editing) output of `history > /tmp/my_session.txt`. This history should include what you typed into the shell outside of the pyspark session. \\[no points for this\\]\n",
    "2. Use DStream windowing to filter incoming headlines. Use a Bloom filter based on `bloom.txt` to emit only the headlines with unfamiliar words in them. Write and submit the (cleaned up after editing) transcript of your session along with your code. \\[4 pts\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.dirname('/home/BigData/PA5/')\n",
    "# to read the bloom filter of known words into memory\n",
    "with open(dir_path + '/streaming/bloom.txt', 'r') as file:\n",
    "    bloomf = file.read()\n",
    "    \n",
    "# # part 1: to create bloom.txt\n",
    "# ! $dir_path'/streaming/headline-extractor.py'\n",
    "\n",
    "# # part 2: to put the bloom.txt into the hdfs\n",
    "! hadoop fs -put -f $dir_path/streaming/bloom.txt hdfs:///user/`whoami`/\n",
    "# ! hadoop fs -ls hdfs:///user/`whoami`/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hash functions used in bloom filteritems_count = len(understood_words) \n",
    "hash_count = 6\n",
    "\n",
    "def bloom_check(item):\n",
    "    '''\n",
    "    Check for existence of an item in filter\n",
    "    if any of bit is does not match, then the word is not present in the filter for sure\n",
    "    else there is a probability that it exists.\n",
    "    \n",
    "    For this particular assignment, because we are required to output unfamiliar words,\n",
    "    this function is designed to throw False when these is a match and True when the \n",
    "    word is not in the dictionary of known words\n",
    "    '''\n",
    "    for seed in range(hash_count):\n",
    "        if bloomf[mmh3.hash(item, seed) % len(bloomf)] == '0':\n",
    "            # NOTE!!!! Reversed intentionally!\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc._conf.getAll()\n",
    "sc.stop()\n",
    "# Create a local StreamingContext with 4 working threads\n",
    "conf = SparkConf().setMaster('local[*]')\n",
    "sc = SparkContext(conf=conf, appName='UnfamiliarWords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-12-11 17:13:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-12-11 17:13:47.500000\n",
      "-------------------------------------------\n",
      "in\n",
      "new\n",
      "for\n",
      "to\n",
      "act\n",
      "on\n",
      "a.g.s\n",
      "ask\n",
      "to\n",
      "two\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-12-11 17:13:50\n",
      "-------------------------------------------\n",
      "f.d.a\n",
      "to\n",
      "on\n",
      "a\n",
      "florida’s\n",
      "data.\n",
      "her\n",
      "home’s\n",
      "raided.\n",
      "midtown\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-12-11 17:13:52.500000\n",
      "-------------------------------------------\n",
      "are\n",
      "dying.\n",
      "did\n",
      "kilar\n",
      "the\n",
      "blow?\n",
      "and\n",
      "california’s\n",
      "the\n",
      "‘trump\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-12-11 17:13:55\n",
      "-------------------------------------------\n",
      "‘the\n",
      "46th’:\n",
      "who\n",
      "in\n",
      "the\n",
      "g.o.p.’s\n",
      "heart?\n",
      "update,\n",
      "on\n",
      "the\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-12-11 17:13:57.500000\n",
      "-------------------------------------------\n",
      "hit:\n",
      "to\n",
      "retirements\n",
      "deb\n",
      "price,\n",
      "a\n",
      "as\n",
      "a\n",
      "on\n",
      "gay\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-12-11 17:14:00\n",
      "-------------------------------------------\n",
      "fallon:\n",
      "hanukkah\n",
      "was\n",
      "a\n",
      "‘festival\n",
      "of\n",
      "lies’\n",
      "the\n",
      "100-year\n",
      "for\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-12-11 17:14:02.500000\n",
      "-------------------------------------------\n",
      "the\n",
      "crossword,\n",
      "tiles\n",
      "and\n",
      "vertex\n",
      "and\n",
      "the\n",
      "a\n",
      "‘war\n",
      "on\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-12-11 17:14:05\n",
      "-------------------------------------------\n",
      "a\n",
      "the\n",
      "and\n",
      "of\n",
      "‘leaf\n",
      "town’\n",
      "go\n",
      "big,\n",
      "and\n",
      "it\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-12-11 17:14:07.500000\n",
      "-------------------------------------------\n",
      "who\n",
      "we\n",
      "be\n",
      "trump?\n",
      "the\n",
      "and\n",
      "the\n",
      "f.d.a.\n",
      "to\n",
      "pfizer's\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-12-11 17:14:10\n",
      "-------------------------------------------\n",
      "the\n",
      "of\n",
      "it’s\n",
      "not\n",
      "you:\n",
      "a\n",
      "is\n",
      "covid-19\n",
      "h.i.v.\n",
      "you\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-12-11 17:14:12.500000\n",
      "-------------------------------------------\n",
      "testing:\n",
      "you\n",
      "to\n",
      "‘small\n",
      "town,\n",
      "no\n",
      "hospital’:\n",
      "covid-19\n",
      "is\n",
      "u.a.e.\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-12-11 17:14:15\n",
      "-------------------------------------------\n",
      "may\n",
      "be\n",
      "for\n",
      "my\n",
      "grandchild's\n",
      "sleepover\n",
      "on\n",
      "to\n",
      "of\n",
      "defects\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-12-11 17:14:17.500000\n",
      "-------------------------------------------\n",
      "in\n",
      "dogged\n",
      "by\n",
      "and\n",
      "estrangement\n",
      "and\n",
      "as\n",
      "anti-vaccine\n",
      "has\n",
      "to\n",
      "...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e166ee683477>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# Start the computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait for the computation to terminate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-12-11 17:14:20\n",
      "-------------------------------------------\n",
      "baricitinib:\n",
      "are\n",
      "of\n",
      "covid-19\n",
      "eli\n",
      "overpaid\n",
      "and\n",
      "a\n",
      "top\n",
      "prods\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-12-11 17:14:22.500000\n",
      "-------------------------------------------\n",
      "the\n",
      "for\n",
      "‘is\n",
      "a\n",
      "survivor?’\n",
      "the\n",
      "oil\n",
      "is\n",
      "at\n",
      "a\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a local StreamingContext with batch interval of 2.5 seconds which should \n",
    "# hold to 10 record per RDD\n",
    "ssc = StreamingContext(sc, 2.5)\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Split each line into words, create a sliding window for each headline, and filter out\n",
    "# the words that are known\n",
    "words = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: word.lower()) \\\n",
    "             .window(windowDuration=2.5, slideDuration=2.5) \\\n",
    "             .filter(bloom_check).pprint()\n",
    "\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
